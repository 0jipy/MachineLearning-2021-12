{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비 지도학습 기반 감성분석 - Lexicon 기반\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet Synset 및 Sentiwordnet SenitSynset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "term = 'present' \n",
    "# 명사로는 선물, 현재 동사로는 표현, 발표\n",
    "synsets = wordnet.synsets(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(synsets), len(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "source": [
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### name: present.n.01 ####\n",
      "POS: noun.time\n",
      "정의: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "표제어: ['present', 'nowadays']\n",
      "#### name: present.n.02 ####\n",
      "POS: noun.possession\n",
      "정의: something presented as a gift\n",
      "표제어: ['present']\n",
      "#### name: present.n.03 ####\n",
      "POS: noun.communication\n",
      "정의: a verb tense that expresses actions or states at the time of speaking\n",
      "표제어: ['present', 'present_tense']\n",
      "#### name: show.v.01 ####\n",
      "POS: verb.perception\n",
      "정의: give an exhibition of to an interested audience\n",
      "표제어: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "#### name: present.v.02 ####\n",
      "POS: verb.communication\n",
      "정의: bring forward and present to the mind\n",
      "표제어: ['present', 'represent', 'lay_out']\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets[:5]:\n",
    "    print(f'#### name: {synset.name()} ####')\n",
    "    print( 'POS:', synset.lexname())\n",
    "    print('정의:', synset.definition())\n",
    "    print('표제어:', synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신셋 클래스의 자료주고. 위와 같은 메소드를 가지고 있는 클래스. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘간의 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiger.n.01 a fierce or audacious person\n",
      "tiger.n.02 large feline of forests in most of Asia having a tawny coat with black stripes; endangered\n"
     ]
    }
   ],
   "source": [
    "# 타이거는 워드넷에 있는 \n",
    "for synset in wordnet.synsets('tiger'):\n",
    "    print(synset.name(), synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어, 품사를 아는 경우에은 synset()\n",
    "tiger = wordnet.synset('tiger.n.02')\n",
    "tree = wordnet.synset('tree.n.01')\n",
    "lion = wordnet.synset('lion.n.01')\n",
    "cat = wordnet.synset('cat.n.01')\n",
    "dog = wordnet.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.16666666666666666, 0.07142857142857142)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어간의 유사도 사자가 타이거와 유사도가 높아. 의미의 유사성 \n",
    "tiger.path_similarity(lion), tiger.path_similarity(dog),tiger.path_similarity(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개 단어간의 상대적인 유사도\n",
    "similarities = []\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "for entity in entities:\n",
    "    similary = [entity.path_similarity(another) for another in entities]\n",
    "    similarities.append(similary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree      lion     tiger       cat       dog\n",
       "0  1.000000  0.066667  0.066667  0.071429  0.111111\n",
       "1  0.066667  1.000000  0.333333  0.250000  0.166667\n",
       "2  0.066667  0.333333  1.000000  0.250000  0.166667\n",
       "3  0.071429  0.250000  0.250000  1.000000  0.200000\n",
       "4  0.111111  0.166667  0.166667  0.200000  1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도를 데이터 프레임으로 만들기 \n",
    "df = pd.DataFrame(similarities, columns=['tree', 'lion', 'tiger', 'cat', 'dog'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentiSynset 객체\n",
    "아까의 Synset 객체와는 다르게 센티라는것을 가지고 있음 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet\n",
    "\n",
    "senti_synsets = list(sentiwordnet.senti_synsets('slow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "7\n",
      "[SentiSynset('mother.n.01'), SentiSynset('mother.n.02'), SentiSynset('mother.n.03'), SentiSynset('mother.n.04'), SentiSynset('mother.n.05'), SentiSynset('mother.v.01'), SentiSynset('beget.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(type(senti_synsets))\n",
    "print(len(senti_synsets))\n",
    "print(senti_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet\n",
    "\n",
    "senti_synsets = list(sentiwordnet.senti_synsets('father'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "9\n",
      "[SentiSynset('father.n.01'), SentiSynset('forefather.n.01'), SentiSynset('father.n.03'), SentiSynset('church_father.n.01'), SentiSynset('father.n.05'), SentiSynset('father.n.06'), SentiSynset('founder.n.02'), SentiSynset('don.n.03'), SentiSynset('beget.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(type(senti_synsets))\n",
    "print(len(senti_synsets))\n",
    "print(senti_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 긍정 감성지수, 부정감성지수, 객관성지수. \n",
    "father = sentiwordnet.senti_synset('father.n.01')\n",
    "father.pos_score(), father.neg_score(), father.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 긍정 감성지수, 부정감성지수, 객관성지수. \n",
    "# senti_synsets = list(sentiwordnet.senti_synsets('mother'))\n",
    "father = sentiwordnet.senti_synset('mother.n.01')\n",
    "father.pos_score(), father.neg_score(), father.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, 0.125, 0.0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 긍정 감성지수, 부정감성지수, 객관성지수. \n",
    "fabulous = sentiwordnet.senti_synset('fabulous.a.01')\n",
    "fabulous.pos_score(), fabulous.neg_score(), fabulous.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('just.a.01'),\n",
       " SentiSynset('equitable.a.01'),\n",
       " SentiSynset('fair.a.01'),\n",
       " SentiSynset('good.s.07'),\n",
       " SentiSynset('merely.r.01'),\n",
       " SentiSynset('precisely.r.01'),\n",
       " SentiSynset('just.r.03'),\n",
       " SentiSynset('just.r.04'),\n",
       " SentiSynset('barely.r.01'),\n",
       " SentiSynset('just.r.06')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 부사는 어떻게 되지 ?\n",
    "list(sentiwordnet.senti_synsets('just'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 졸리면 \n",
    "# work 단어의 긍정감성 지수, 부정감성 지수, 객관성 지수 동사는 감성지수 값이 없다. \n",
    "# 그럴줄 알았는데 이걸 용재가??? \n",
    "work = sentiwordnet.senti_synset('work.v.01')\n",
    "work.pos_score(), work.neg_score(), work.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.0, 0.5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love = sentiwordnet.senti_synset('love.v.01')\n",
    "love.pos_score(), love.neg_score(), love.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('n', 'a', 'r', 'v')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.NOUN, wordnet.ADJ, wordnet.ADV, wordnet.VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 감성지수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tag import pos_tag\n",
    "# sentence = \"It's good to see you again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', \"'s\", 'good', 'to', 'see', 'you', 'again', '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "sentence = \"It's good to see you again.\"\n",
    "word_list = word_tokenize(sentence)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('again', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_list)\n",
    "# 워드넷과의 차이 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = ('good', 'JJ')\n",
    "tag[1].startswith('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 508p 워드넷 품사 태그로 변환 \n",
    "# what is J\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good a\n",
      "see n\n",
      "you None\n",
      "again r\n"
     ]
    }
   ],
   "source": [
    "for word, tag in pos_tag(word_list):\n",
    "    print(word, penn_to_wn(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 태그 자리에 penn_to_wn???\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'see', 'you', 'again']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence로부터 Senti_Synset 객체를 만드는 과정 \n",
    "sentence = \"It's good to see you again.\"\n",
    "word_list = [word for word in word_tokenize(sentence) if len(word) > 2]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<good.a.01: PosScore=0.75 NegScore=0.0>\n",
      "<see.n.01: PosScore=0.0 NegScore=0.0>\n",
      "<again.r.01: PosScore=0.0 NegScore=0.0>\n"
     ]
    }
   ],
   "source": [
    "for word, tag in pos_tag(word_list):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(sentiwordnet.senti_synsets(word, wn_tag))\n",
    "        synset = synsets[0]\n",
    "        print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = 0\n",
    "\n",
    "for word, tag in pos_tag(word_list):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(sentiwordnet.senti_synsets(word, wn_tag))\n",
    "        synset = synsets[0]\n",
    "        sentiment += synset.pos_score() - synset.neg_score()\n",
    "sentiment\n",
    "\n",
    "# 해석좀.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = 0\n",
    "for word, tag in pos_tag(word_list):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "        synsets = list(sentiwordnet.senti_synsets(lemma, wn_tag))\n",
    "        synset = synsets[0]\n",
    "        sentiment += synset.pos_score() - synset.neg_score()\n",
    "sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 위 어떤 데이터로 할 끄나. ? \n",
    "# labeledTrainData.tsv 요놈으로 간다\n",
    "# 06.Text분석\\data\\labeledTrainData.tsv 의 9번째 줄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "document = \"I watched this video at a friend's house. I'm glad I did not waste money buying this one. The video cover has a scene from the 1975 movie Capricorn One. The movie starts out with several clips of rocket blow-ups, most not related to manned flight. Sibrel's smoking gun is a short video clip of the astronauts preparing a video broadcast. He edits in his own voice-over instead of letting us listen to what the crew had to say. The video curiously ends with a showing of the Zapruder film. His claims about radiation, shielding, star photography, and others lead me to believe is he extremely ignorant or has some sort of ax to grind against NASA, the astronauts, or American in general. His science is bad, and so is this video.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene\n",
      "blow-ups\n",
      "Sibrel\n",
      "voice-over\n",
      "Zapruder\n",
      "others\n",
      "부정\n"
     ]
    }
   ],
   "source": [
    "sentiment = 0.0\n",
    "for sentence in sent_tokenize(document):\n",
    "    word_list = word_list = [word for word in word_tokenize(sentence) if len(word) > 2]\n",
    "    for word, tag in pos_tag(word_list):\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag:\n",
    "            lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "            synsets = list(sentiwordnet.senti_synsets(lemma, wn_tag))\n",
    "            if not synsets:\n",
    "                print(word)\n",
    "                continue\n",
    "            synset = synsets[0]\n",
    "            sentiment += synset.pos_score() - synset.neg_score()\n",
    "print('긍정긍정' if sentiment >=0 else '부정')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤놈이 괴롭혔나..  # 어기 신셋츠에서 뻑난놈 찾아서 어케 알아보지 ???\n",
    "# scene\n",
    "# blow-ups\n",
    "# Sibrel\n",
    "# voice-over\n",
    "# Zapruder\n",
    "# others\n",
    "# -1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 감성을 계산해주는 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 깃허브에 올라와 있어.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swn_polarity(text):\n",
    "    # 감성 지수 초기화 \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산 \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NTLK 기반의 품사 태깅 문장 추출  \n",
    "        word_list = [word for word in word_tokenize(raw_sentence) if len(word) > 2]\n",
    "        tagged_sentence = pos_tag(word_list)\n",
    "        for word, tag in tagged_sentence:\n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wordnet.NOUN, wordnet.ADJ, wordnet.ADV, wordnet.VERB):\n",
    "                continue                   \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성. \n",
    "            synsets = wordnet.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            # 모든 단어에 대해 긍정 감성 지수는 +로 부정 감성 지수는 -로 합산해 감성 지수 계산. \n",
    "            synset = synsets[0]\n",
    "            swn_synset = sentiwordnet.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())           \n",
    "            tokens_count += 1\n",
    "    \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    # 총 score가 0 이상일 경우 긍정(Positive) 1, 그렇지 않을 경우 부정(Negative) 0 반환\n",
    "    return 1 if sentiment >= 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', sep='\\t', quoting=3) # 3 :  Quote - None\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.str.replace('<br />', ' ')\n",
    "df.review = df.review.str.replace('[^A-Za-z]', ' ').str.strip()\n",
    "# 공백제거. 숫자및 구두점 삭제 영문만 남도록 \n",
    "# df.review[0][:1000]\n",
    "# 데이터 25000개 시간 10분이상 걸릴것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.iloc[:1000, :]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%time df['pred'] = df.review.apply(lambda x: swn_polarity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.622"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df.sentiment, df.pred)\n",
    "# 성능은 그닥좋지 못하지만 렉시콘 사전이용 비지도학습 감성 분석. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지도학습 기계학습 써야한다. 왜냐 어렵잖아.. 지도학습 괜찮은데 반해서 \n",
    "센티워드넷. 이용 영화감상평 분석한것. 매땅에 헤딩하려니까 되게 어려워. \n",
    "어려운데 성능도 안좋아   \n",
    "하는 이유 없어 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANDER Lexicon을 이용한 감성 분석\n",
    "    - 센티워드넷은 바닥에 있는 놈 이용한것. 워드넷 렘마타이져 토크나이저 등등 다 가져와\n",
    "    - 앞으로는 하이레벨 Vander Lexicon 이용 .. 쫄지마 앞으론 이것만 쓰니까 \n",
    "    - 텐서플로우 위에 케라스 .케세라세라세라 파워모드 가야 \n",
    "    - 텐서플로우로 딥러닝을 해라.... 딥러닝 커터. nltk 사용법보다 무지무지 어려워\n",
    "\n",
    "    level 1,2,3,4 중에서 레벨4되야 텐서플로우 쓸 수 있어. 그만큼 프로그래밍 어려워 .\\\n",
    "    그래서 케라스가 나왔어. 이것도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "senti_score = senti_analyzer.polarity_scores(df.review[0])\n",
    "senti_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_polarity(document, threshold = 0.1):\n",
    "    score = senti_analyzer.polarity_scores(document)\n",
    "    return 1 if score['compound'] >= threshold else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%time df['vader pred'] = df.review.apply(lambda x: vader_polarity(x, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69556"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위는 시간도 빠르고 빠르고 결과도 더 좋아. 걍 써라 두번 써라 머겅. \n",
    "accuracy_score(df.sentiment, df['vader pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6분 해서 끝났다.  # 임계값 0.1로 주니 1분으로 줄었어. 예측비교 값 \n",
    "# 근데 도대체 콤보는 어디에 뜨는겨 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예측 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>pred</th>\n",
       "      <th>vader pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  pred  vader pred\n",
       "0          1     1           0\n",
       "1          1     1           1\n",
       "2          0     0           0\n",
       "3          0     0           1\n",
       "4          1     0           1\n",
       "5          1     0           1\n",
       "6          0     1           0\n",
       "7          0     0           0\n",
       "8          0     0           1\n",
       "9          1     1           1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = df[['sentiment', 'pred', 'vader pred']]\n",
    "cdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
