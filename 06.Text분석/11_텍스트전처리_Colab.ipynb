{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.텍스트전처리-Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리 (Preprocsssing)"
      ],
      "metadata": {
        "id": "Bv5859EPk6eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cpu 순행식 gpu 돈천만원 강의 속도"
      ],
      "metadata": {
        "id": "l9YkNQM0mkqd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 토큰화(tokenization)\n",
        "    - 말뭉치가 있을 때 도큐먼트로 나누고, 도큐먼트 안에서 \n",
        "    - 여기서는 보통 문장 / 워드 / 우리나라 말 경우에는 형태소. \n",
        "    - 토큰화 정제(클리닝,클렌징) 정규화 등의 순서로 이루어 진다 .\n",
        "    - 영어는 띄어쓰기 단위로 단어 토큰화. \n",
        "\n",
        "    - 다양한 선택지 어퍼스트로키 코기코기코기코기코기 왓이댓 "
      ],
      "metadata": {
        "id": "85b2Bam_m6_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUFpqopXoB3H",
        "outputId": "ccffa953-42ea-4358-d1b6-855c53f06433"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "KkwcCxIqnYoK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sample))\n",
        "# 돈트 표시 확인 구두점 토큰 간주. 's   . 마침표 등 토큰화 됨. 자세한 내용은 생략한다. \n",
        "# 워드 토크나이저 피처지는 유혈사태 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1zHt2tSpYE7",
        "outputId": "37508ab1-8ed5-4665-9647-144d12fefbf0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 소문자 함수인데 반해 아래 대문자 워드 토크나이저는 객체 . 고로 객체 생성후 사용할것 !!!!\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "print(wpt.tokenize(sample))\n",
        "# "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5PjvJCipqaB",
        "outputId": "2042f059-8a05-490e-9b08-050ef0ffb90a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞으로 딥러닝에는 케라스 사용할 것\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(sample))\n",
        "# 두낫을 분리않고 다 소문자 바꿔주고 구두점은 분리 않고 등등 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ygs0KpkqNUk",
        "outputId": "666fa2de-2110-4610-bb3b-838a5256f3fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "# from nltk.tokenize import TreebankWordTokenizer\n",
        "tok = TreebankWordTokenizer()\n",
        "print(tok.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4jaEmDKqhYJ",
        "outputId": "de011248-082b-4a39-e35d-5806398a32ed"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 문장토큰화"
      ],
      "metadata": {
        "id": "B4SJxZuYrcb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장단위로 구분되어지는 [] 리스트 생겨. 이거 구두점. 마침표 단위로 나누는 겨?\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeWgv2pwrboA",
        "outputId": "653bebb0-453c-44a0-f17a-591cce90fd04"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in  sent_tokenize(text):\n",
        "    print(word_tokenize(sentence))\n",
        "# 문장단위 나누고, 그것을 단어단위로 토크화 햇기에 아래처럼 5문장 리스트. 포문 돌것. 어후 정신사나와. ㅠㅠ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFzGp6dCsJjB",
        "outputId": "7a6a86b4-82ea-443a-82f2-5637590c9839"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His', 'barber', 'kept', 'his', 'word', '.']\n",
            "['But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy', '.']\n",
            "['Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff', '.']\n",
            "['He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds', '.']\n",
            "['He', 'looked', 'about', ',', 'to', 'make', 'sure', 'no', 'one', 'was', 'near', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다시 확인할것.  # 신기하게도.  마침표. 단위라고 해도 ph.D.  같은 줄임말 형태로 분리되지 않았어. \n",
        "text = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJlMPRXxslgN",
        "outputId": "67b5f5f1-cae9-41aa-c4d2-0065c84bc450"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dgpToHjwtK5Z"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글문장 토큰화\n",
        "    - 띄어쓰기가 어절단위로 이루어지는데, \n",
        "    - 단어 단위의 토근화를 목표로 하고 있어서 한국어는 토큰화가 어렵다. \n",
        "    - 한국어가 교착어 기 때무인데. 그렇기에 한국어에서 토근화는 형태소 개념을 반드시 이해해야 한다. \n",
        "    - 교착어의 특성. 어절단위 띄어쓰기, 조사가 붙어서 형성. 같은 단어임에도 조사가 달라져 다른 단어로 인식되는 특성. 그러므로 한국어에서는 조사를 분리해야 하고 . 이것을 형태소를 기반으로 하기에 영어와 다르게 어려운점ㅇ 있어. "
      ],
      "metadata": {
        "id": "yd9Xsci0tNGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(Korean Sentence Splitter)\n",
        "!pip install kss\n",
        "# 설치했을때 메시지 않보고 싶다. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU3eZi2CtMAG",
        "outputId": "5f92dded-0ee3-4370-e4ac-7c953855ceed"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (3.3.1.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss) (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(Korean Sentence Splitter) 설치시 메세지를 보고싶지 않을 때 \n",
        "!pip install kss > /dev/null"
      ],
      "metadata": {
        "id": "clBvjN0etUpU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5_tzVVawxN0",
        "outputId": "84cb9a27-89d2-4fe8-9b49-c0989dc6d331"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 55504\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rw-r--r-- 1 root root   301141 Dec 23 14:32 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Dec 23 14:32 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Dec 23 14:32 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Dec 23 14:32 mnist_train_small.csv\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9hHcP36w1XN",
        "outputId": "48e8b4cc-8322-49de-a4e5-7920074b4f10"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자립 형태소와 의존 형태소 \n",
        "접사, "
      ],
      "metadata": {
        "id": "-I4L3rjQyHfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3)품사 태깅(part-of-speech tagging)"
      ],
      "metadata": {
        "id": "TV2y5Qwp1aNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gl4uS4Y1P4o",
        "outputId": "a0764476-1b00-4078-ac62-564f72982cea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ake5Bx9w1gD4",
        "outputId": "e9326243-44b3-4a92-fac2-5e31b971b896"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(word_tokenize(text))\n",
        "# 단어의 품사까지 나타내주는 품사 태깅. 줄임말이 무엇인지 확인 할 필요 있어. \n",
        "# NLP는 고유 명사 \n",
        "# '''\n",
        "# 영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. \n",
        "# Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\n",
        "# 한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\n",
        "# 한국어 NLP에서 형태소 분석기를 사용하여 단어 토큰화. 더 정확히는 형태소 토큰화(morpheme tokenization)를 수행해보겠습니다. 여기서는 Okt와 꼬꼬마 두 개의 형태소 분석기를 사용하여 토큰화를 수행하겠습니다.\n",
        "# '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "7xD6oRaR1r3o",
        "outputId": "9b0ba983-77a5-4c14-cf5f-ad2f7ec64784"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\\n\\n한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\\n\\n한국어 NLP에서 형태소 분석기를 사용하여 단어 토큰화. 더 정확히는 형태소 토큰화(morpheme tokenization)를 수행해보겠습니다. 여기서는 Okt와 꼬꼬마 두 개의 형태소 분석기를 사용하여 토큰화를 수행하겠습니다.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 품사 태깅 설명\n",
        "\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.tag import pos_tag\n",
        "\n",
        "# text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "# tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "# print('단어 토큰화 :',tokenized_sentence)\n",
        "# print('품사 태깅 :',pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mfy5ZQA1M-5",
        "outputId": "c235eec7-15d0-45fd-cc84-c5b90f7b147a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "형태소는 morpheme 뜻의미를 가진 가장 작은 말의 단위. \n",
        "두가지의 형태소, 자립 / 의존 \n",
        "자립형태소, 그자체로 사용한ㄹ수 있어서 단어가 된다. 체언 (명사, 대명사, 수사)"
      ],
      "metadata": {
        "id": "0xuuHZE82exw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 KoNLPy : 코엔엘파이"
      ],
      "metadata": {
        "id": "p8wtSI2o1j6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy 설치하기\n",
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PirKnGX3dqz",
        "outputId": "42ad61d8-2b46-4377-9f74-9d0845f7af60"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 485 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->Konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, Konlpy\n",
            "Successfully installed JPype1-1.3.0 Konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okt(Open Korean Text)\n",
        " - 형태소 분석. text.morphs 분석시 stem 줄기옵션 트루루 주면 동사의 경우는 원형이 나온다. 한글이니 다른 형태도 확인해 보자. \n",
        " - 품사태깅 text.pos\n",
        " - 명사추출 text.nouns\n"
      ],
      "metadata": {
        "id": "1ZmQBCKc4lUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OKT 실습\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNWDfaen3mFc",
        "outputId": "ba5d961c-22d1-4af5-8e7d-aeb2d27c4b8a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt.morphs(text, stem=True)  #동사의 경우는 원형이 나와  # 가보다가 원형?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS5Wp0B_4hUw",
        "outputId": "73e51d7e-710f-4f38-d819-024ac32ef594"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 부착\n",
        "okt.pos(text)\n",
        "# PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shXX4z1249yF",
        "outputId": "f8c9826e-5d1b-4736-9255-1592d3a84c45"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사추출\n",
        "okt.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEpv5Two5J6k",
        "outputId": "68cc48dc-71f8-462d-c586-acdd91c19f8b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기까지 okt "
      ],
      "metadata": {
        "id": "ivLhWqyA5aCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "꼬꼬마 "
      ],
      "metadata": {
        "id": "yucSJ0Eu571_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma # 처음 대문자 주의\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)\n",
        "# 하 ㄴ 가보 아요 등등 차이점 있어. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td-_c0Z65-bG",
        "outputId": "8346323a-65c7-4409-e179-06e47163ebef"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요'] 오케이 티의 경우와 비교"
      ],
      "metadata": {
        "id": "jA579F9K6PZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW_eD_Hz6KhX",
        "outputId": "fbff4c53-9fc8-4b78-c558-96635be4fbb6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'MAG'),\n",
              " ('코딩', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㄴ', 'ETD'),\n",
              " ('당신', 'NP'),\n",
              " (',', 'SP'),\n",
              " ('연휴', 'NNG'),\n",
              " ('에', 'JKM'),\n",
              " ('는', 'JX'),\n",
              " ('여행', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('가보', 'VV'),\n",
              " ('아요', 'EFN')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 꼬꼬마 명사추출 \n",
        "kkma.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLO4rJjo6Yvy",
        "outputId": "6996f231-cd05-41d9-bc12-29437774fee9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 고민없이 아무거나 잡아쓰면 될것. "
      ],
      "metadata": {
        "id": "Wh-Jmxnf6cTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 정제(Clening) 와 정규화 (Nomalization)\n",
        " - 1. 규칙에 기반한 표기가 다른 단어들의 통합. 어간추출 / 표제어 추출(렘마타이제이션)\n",
        " - 2. 대소문자 통합 (미국 US 와 우리us는 달라야 한다. 주의 )\n",
        " - 3. 불필요한 단어의 제거 \n",
        "        - 1. 등장빈도가 적은 단어. \n",
        "        - 2. 길이가 짧은 단어 \n",
        "            : 영어에서 평균단어길이 6-7 추정 한국어는 2-3 추정. 그래서 영어는 3개부터 제거할때고려가 필요하고, 한국어는 음.. .애초에 고려가 필요해 200 콤보완성!!\n",
        "        -3 어쨋든 두글자 짜리는 줄여도 괜츈혀. 파워코딩파워코딩.파워코딩.\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "lcnLHHcB6wYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규표현식 ㄱㄱ\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\""
      ],
      "metadata": {
        "id": "Ero76um__pIr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "shortword.sub('', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "54SLesV5_sD_",
        "outputId": "4765cf1d-ac52-4bcd-dd25-22ea91b2063c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' was wondering anyone out there could enlighten this car.'"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 우리에게 편한게 뭘까 ?\n",
        "[word for word in word_tokenize(text) if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uT8ZOw7_0NS",
        "outputId": "c00b837d-2211-4aa2-82d0-1718715b3df4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = ' '.join([word for word in word_tokenize(text) if len(word) > 2])\n",
        "clean_text\n",
        "# 결과는 길이 2개 이하 단어 제거 후 다시 붙여서 조인. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qtYpTgAA_4Hf",
        "outputId": "454471b5-b25b-4832-b10c-d735413cf007"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was wondering anyone out there could enlighten this car'"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화한 후 길이가 2보다 큰단어만 발췌\n",
        "[word for word in word_tokenize(text) if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VMvV2I-AYC8",
        "outputId": "26493908-986e-4833-d2b7-7ff1665637c0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 어간추출(Stemming) 및 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "5AA0uVoGA3d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손구랑이 장나닞ㄹ이냐 첫번째ㅗㄹ"
      ],
      "metadata": {
        "id": "nV-GzrdmBTsG"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 표제어 추출에서 \n",
        " 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것입니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문을 뜻합니다. 형태소의 종류로 어간(stem)과 접사(affix)가 존재합니다.\n",
        "\n",
        " - NLTK에서는 표제어 추출을 위한 도구로 WordNetLemmatizer를 지원한다 .ㄱㄱ\n",
        " - 표제어추출램마타이즈 단어 뒤에 원형까지 붙여주면 잘한다 .짝짝짝"
      ],
      "metadata": {
        "id": "_G_zpxG6Baxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73lvjkYNB171",
        "outputId": "a1c32f23-65a5-4fd4-ffc7-a1b9f5d248dc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "d1EFkw2vCDkE"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lemma.lemmatize(word) for word in words])\n",
        "\n",
        "# print('표제어 추출 전 :',words)\n",
        "# print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwaPVuqsCZ_6",
        "outputId": "69a5bee9-1a52-4120-fbab-473017bad83e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('lives', 'v'), lemma.lemmatize('dies', 'v'), lemma.lemmatize('watched', 'v'), lemma.lemmatize('has', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKOBuuUbC8aT",
        "outputId": "2ce9077a-cfdf-48f3-8040-a3ac61827e92"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('live', 'die', 'watch', 'have')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사에 따른 원형. \n",
        "lemma.lemmatize('lives', 'v'), lemma.lemmatize('lives', 'n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2thHeCI7Doux",
        "outputId": "0633583d-2a78-462a-b2af-b59717d3cb94"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('live', 'life')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 어간추출 (stemming)\n",
        " - 다아시 댄스 라이킷 "
      ],
      "metadata": {
        "id": "XQp38lZMD5cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "2eo9shsDEO5S"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PouokR36ErMp",
        "outputId": "bbd82f13-3f4c-42d1-9f3e-2298bc5e4973"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 후 \n",
        "print([ps.stem(word) for word in word_tokenize(text)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rbtm9k8EvSg",
        "outputId": "237fda5a-5a14-4931-a500-5a5891e8c4bc"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([ps.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy2yOu9BEvP5",
        "outputId": "3de3285e-e13f-4c2e-9a67-31ade0cfa054"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer ?? \n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([ps.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M4f51pZEvMn",
        "outputId": "20d3b45b-3950-4a06-dd2c-1d1d360843ab"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print([ls.stem(word) for word in words]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVW2LHiXEvJw",
        "outputId": "7ff66faf-fe84-49ab-b985-8b371d25e5c0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yzJibOjmF5KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem import PorterStemmer\n",
        "# from nltk.stem import LancasterStemmer\n",
        "\n",
        "# porter_stemmer = PorterStemmer()\n",
        "# lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "# words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "# print('어간 추출 전 :', words)\n",
        "# print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
        "# print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
      ],
      "metadata": {
        "id": "c-XsQvdRETeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "# 포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
        "# 랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']"
      ],
      "metadata": {
        "id": "_0csE6C0EbfL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}